{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "The primary objective of this project was to design, train, and deploy a deep learning-based classification model capable of detecting brain tumor types from MRI images. Early and accurate diagnosis of brain tumors is crucial for effective treatment and patient survival. Leveraging machine learning and computer vision, this project aims to assist radiologists and medical professionals by automating the classification of four tumor classes: glioma, meningioma, pituitary, and no tumor.\n",
        "\n",
        "The dataset used was the publicly available Brain Tumor MRI Dataset, organized into training, validation, and test sets across four classes. The images were of consistent resolution and quality, with no corrupt or duplicate files. Initial exploratory data analysis confirmed a moderate class imbalance, which was addressed using data augmentation techniques such as rotation, zoom, flipping, and brightness adjustments. These augmentations enriched the training set, improving the model’s generalization.\n",
        "\n",
        "Two modeling approaches were pursued: a Custom CNN built from scratch and a Transfer Learning model using MobileNetV2. The custom CNN consisted of multiple convolutional and pooling layers with dropout and batch normalization to reduce overfitting. It achieved a validation accuracy of approximately 68% and a test accuracy of 69.1%. While promising, it faced challenges in generalizing certain tumor classes.\n",
        "\n",
        "To enhance performance, a MobileNetV2 model pretrained on ImageNet was fine-tuned with a custom classification head tailored to our four-class problem. Transfer learning significantly boosted performance, achieving a validation accuracy of 79% and a test accuracy of 79.2%. The classification report showed high precision and recall across most classes, especially for glioma and no_tumor, while meningioma posed moderate challenges, likely due to feature similarity with other classes.\n",
        "\n",
        "Model evaluation involved metrics such as accuracy, precision, recall, F1-score, and a confusion matrix, which helped visualize prediction distribution and misclassifications. These metrics were critical in selecting the final model for deployment. Based on performance and generalization, MobileNetV2 was chosen as the final model due to its better accuracy, faster convergence, and lower computational cost compared to the custom CNN.\n",
        "\n",
        "For deployment, the best performing model was saved in .h5 format and wrapped in a user-friendly Streamlit web application. The app allows users to upload MRI images and get real-time predictions with confidence scores. It also displays class-wise probability breakdowns, enhancing transparency and usability for non-technical users like healthcare professionals.\n",
        "\n",
        "This end-to-end project—from data preparation, modeling, evaluation, and deployment—demonstrates a practical application of deep learning in medical imaging. The solution provides a scalable, accessible, and interpretable diagnostic aid for radiologists, especially in resource-constrained settings.\n",
        "\n",
        "Through this project, I gained hands-on experience in data wrangling, CNN architecture design, transfer learning, model evaluation, and web app deployment using Streamlit. I also understood the real-world importance of evaluation metrics in medical domains, where false negatives and positives can have life-altering consequences.\n",
        "\n",
        "In conclusion, this brain tumor classification project showcases how deep learning can significantly assist in medical diagnostics. The integration of performance-tuned models with intuitive deployment pipelines like Streamlit bridges the gap between complex machine learning solutions and impactful, real-world applications in healthcare.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "https://github.com/RudraAhuja/Brain_Tumor_Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "This project aims to develop a deep learning-based solution for classifying brain MRI images into multiple categories according to tumor type. It involves building a custom CNN model from scratch and enhancing performance through transfer learning using pretrained models. The project also includes deploying a user-friendly Streamlit web application to enable real-time tumor type predictions from uploaded MRI images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Basic Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "import cv2\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50, MobileNetV2, InceptionV3, EfficientNetB0\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Miscellaneous\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path to the Tumour dataset folder\n",
        "DATASET_DIR = '/content/drive/MyDrive/Tumour'\n",
        "\n",
        "# Check contents of train, valid, and test folders\n",
        "import os\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    path = os.path.join(DATASET_DIR, split)\n",
        "    if os.path.exists(path):\n",
        "        print(f\"{split} folder found with {len(os.listdir(path))} class folders.\")\n",
        "    else:\n",
        "        print(f\"{split} folder NOT found.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_images(path):\n",
        "    for cls in os.listdir(path):\n",
        "        cls_path = os.path.join(path, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            num_images = len(os.listdir(cls_path))\n",
        "            print(f\"{cls}: {num_images} images\")\n",
        "\n",
        "print(\"📁 Training Set:\")\n",
        "count_images(os.path.join(DATASET_DIR, 'train'))\n",
        "\n",
        "print(\"\\n📁 Validation Set:\")\n",
        "count_images(os.path.join(DATASET_DIR, 'valid'))\n",
        "\n",
        "print(\"\\n📁 Test Set:\")\n",
        "count_images(os.path.join(DATASET_DIR, 'test'))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "train_path = os.path.join(DATASET_DIR, 'train')\n",
        "class_names = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, cls in enumerate(class_names[:4]):  # Show 1 image from each of the first 4 classes\n",
        "    img_dir = os.path.join(train_path, cls)\n",
        "    img_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if img_files:  # Only if images exist\n",
        "        img_path = os.path.join(img_dir, img_files[0])\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(cls)\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_total_images_and_classes(data_dir):\n",
        "    total_images = 0\n",
        "    class_names = []\n",
        "\n",
        "    for cls in os.listdir(data_dir):\n",
        "        cls_path = os.path.join(data_dir, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            class_names.append(cls)\n",
        "            total_images += len(os.listdir(cls_path))\n",
        "\n",
        "    return total_images, len(class_names), class_names\n",
        "\n",
        "splits = ['train', 'valid', 'test']\n",
        "\n",
        "for split in splits:\n",
        "    path = os.path.join(DATASET_DIR, split)\n",
        "    total_imgs, num_classes, classes = count_total_images_and_classes(path)\n",
        "    print(f\"\\n📁 {split.upper()} SET\")\n",
        "    print(f\"Total Images: {total_imgs}\")\n",
        "    print(f\"Number of Classes: {num_classes}\")\n",
        "    print(f\"Classes: {classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def dataset_info(split_path):\n",
        "    print(f\"\\n🔍 Dataset Info for: {split_path}\")\n",
        "    class_names = [d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))]\n",
        "    print(f\"Number of Classes: {len(class_names)}\")\n",
        "    print(\"Class Names:\", class_names)\n",
        "\n",
        "    total_images = 0\n",
        "    img_shapes = []\n",
        "\n",
        "    for cls in class_names:\n",
        "        cls_path = os.path.join(split_path, cls)\n",
        "        image_files = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        total_images += len(image_files)\n",
        "\n",
        "        # Collect image size for the first image\n",
        "        if image_files:\n",
        "            img = Image.open(os.path.join(cls_path, image_files[0]))\n",
        "            img_shapes.append(img.size)\n",
        "\n",
        "        print(f\"{cls}: {len(image_files)} images\")\n",
        "\n",
        "    print(f\"Total Images: {total_images}\")\n",
        "    if img_shapes:\n",
        "        print(f\"Sample Image Sizes: {img_shapes[:3]}\")  # Show sizes of 3 samples\n",
        "\n",
        "# Run for each dataset split\n",
        "dataset_info(os.path.join(DATASET_DIR, 'train'))\n",
        "dataset_info(os.path.join(DATASET_DIR, 'valid'))\n",
        "dataset_info(os.path.join(DATASET_DIR, 'test'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "\n",
        "def find_duplicate_images(dataset_path):\n",
        "    hash_map = defaultdict(list)\n",
        "\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                img_path = os.path.join(class_path, filename)\n",
        "                with Image.open(img_path) as img:\n",
        "                    img_hash = hashlib.md5(img.tobytes()).hexdigest()\n",
        "                    hash_map[img_hash].append(img_path)\n",
        "\n",
        "    # Report duplicates\n",
        "    duplicate_count = 0\n",
        "    print(f\"\\n🔁 Duplicate Images in: {dataset_path}\")\n",
        "    for h, paths in hash_map.items():\n",
        "        if len(paths) > 1:\n",
        "            duplicate_count += len(paths) - 1\n",
        "            print(f\"\\nDuplicate group ({len(paths)} copies):\")\n",
        "            for p in paths:\n",
        "                print(\"  -\", p)\n",
        "\n",
        "    print(f\"\\n🧮 Total Duplicates Found: {duplicate_count}\")\n",
        "\n",
        "# Run on each dataset split\n",
        "find_duplicate_images(os.path.join(DATASET_DIR, 'train'))\n",
        "find_duplicate_images(os.path.join(DATASET_DIR, 'valid'))\n",
        "find_duplicate_images(os.path.join(DATASET_DIR, 'test'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "def check_missing_or_corrupt_images(dataset_path):\n",
        "    missing_count = 0\n",
        "    total_files = 0\n",
        "    non_images = 0\n",
        "\n",
        "    print(f\"\\n🔍 Checking: {dataset_path}\")\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, filename)\n",
        "            total_files += 1\n",
        "\n",
        "            if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                non_images += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()  # verifies if the image is not broken\n",
        "            except (UnidentifiedImageError, IOError, OSError):\n",
        "                print(f\"❌ Corrupt or unreadable image: {file_path}\")\n",
        "                missing_count += 1\n",
        "\n",
        "    print(f\"📦 Total Files Checked: {total_files}\")\n",
        "    print(f\"🧊 Non-image Files: {non_images}\")\n",
        "    print(f\"⚠️ Corrupt/Missing Images: {missing_count}\")\n",
        "\n",
        "# Run on each split\n",
        "check_missing_or_corrupt_images(os.path.join(DATASET_DIR, 'train'))\n",
        "check_missing_or_corrupt_images(os.path.join(DATASET_DIR, 'valid'))\n",
        "check_missing_or_corrupt_images(os.path.join(DATASET_DIR, 'test'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "##  Dataset Summary: Brain Tumor MRI Classification\n",
        "\n",
        "###  Dataset Structure\n",
        "- The dataset is organized into three folders: `train`, `valid`, and `test`.\n",
        "- Each split contains subfolders representing tumor classes (e.g., `glioma`, `meningioma`, `pituitary`, etc.).\n",
        "- Images are stored directly inside each class folder.\n",
        "\n",
        "###  Class Distribution\n",
        "- Multiple tumor classes exist.\n",
        "- Each class contains a different number of images, indicating some class imbalance.\n",
        "- We explored and visualized sample images per class.\n",
        "\n",
        "###  Data Quality Checks\n",
        "-  **Missing Images**: No missing or unreadable images were found.\n",
        "- **Duplicate Images**: No exact duplicate images detected using hashing.\n",
        "-  **Non-image Files**: None detected during preprocessing.\n",
        "\n",
        "###  Image Information\n",
        "- Images are in `.jpg`, `.jpeg`, or `.png` formats.\n",
        "- Image resolutions vary and will be resized to a consistent shape (e.g., 224×224) during preprocessing.\n",
        "- Some images are grayscale or RGB — this will be standardized in preprocessing.\n",
        "\n",
        "###  Conclusion\n",
        "The dataset is clean, well-structured, and ready for deep learning preprocessing and modeling.\n",
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        " #This dataset doesn't have columns — it's folder-based, not a tabular (CSV) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def describe_dataset(split_path):\n",
        "    print(f\"\\n🔍 Dataset Info for: {split_path}\")\n",
        "\n",
        "    class_names = [d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))]\n",
        "    print(f\"Number of Classes: {len(class_names)}\")\n",
        "    print(\"Class Names:\", class_names)\n",
        "\n",
        "    total_images = 0\n",
        "    sample_shapes = []\n",
        "\n",
        "    for cls in class_names:\n",
        "        cls_path = os.path.join(split_path, cls)\n",
        "        image_files = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        total_images += len(image_files)\n",
        "\n",
        "        if image_files:\n",
        "            img_path = os.path.join(cls_path, image_files[0])\n",
        "            with Image.open(img_path) as img:\n",
        "                sample_shapes.append(img.size)\n",
        "\n",
        "        print(f\"{cls}: {len(image_files)} images\")\n",
        "\n",
        "    print(f\"Total Images: {total_images}\")\n",
        "    if sample_shapes:\n",
        "        print(\"Sample Image Sizes:\", sample_shapes[:3])\n",
        "\n",
        "# Run this for each dataset split\n",
        "describe_dataset('/content/drive/MyDrive/Tumour/train')\n",
        "describe_dataset('/content/drive/MyDrive/Tumour/valid')\n",
        "describe_dataset('/content/drive/MyDrive/Tumour/test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "DATASET_DIR: This is the root path to your dataset folder, typically something like /content/drive/MyDrive/Tumour. It contains subfolders named train, valid, and test.\n",
        "\n",
        "split: Used in a loop to refer to each dataset subset – either 'train', 'valid', or 'test'.\n",
        "\n",
        "path: Refers to the full path to each split folder, e.g., /content/drive/MyDrive/Tumour/train.\n",
        "\n",
        "cls: Represents the name of a class folder, such as glioma, meningioma, no_tumor, or pituitary.\n",
        "\n",
        "cls_path: The full path to a specific class folder, combining the split path and the class name.\n",
        "\n",
        "img_files: A list of image filenames in a particular class folder, used to count or preview them.\n",
        "\n",
        "img_path: Full path to an individual image file used for reading or displaying.\n",
        "img: Represents the loaded image, either as a PIL image or a NumPy array depending on how it's read.\n",
        "\n",
        "total_images: Keeps track of the number of images found in a dataset split.\n",
        "\n",
        "class_names: A list of the class folder names found inside each dataset split.\n",
        "\n",
        "img_shapes: A list that stores the dimensions of a few sample images, used to confirm consistency in image sizes.\n",
        "\n",
        "sample_shapes: Similar to img_shapes, used for printing image sizes of a few example images per class.\n",
        "\n",
        "filename: Refers to the name of an individual image file being processed.\n",
        "\n",
        "file_path: Full path to the image file being checked for validity or corruption.\n",
        "\n",
        "hash_map: A dictionary used to store hashes of image byte data, helping to detect duplicate images.\n",
        "\n",
        "img_hash: The MD5 hash of a given image, used for checking duplicates.\n",
        "\n",
        "non_images: A counter that keeps track of files that are not image types.\n",
        "\n",
        "missing_count: Counts how many image files are unreadable, corrupt, or missing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "DATASET_DIR = '/content/drive/MyDrive/Tumour'\n",
        "\n",
        "def check_unique_values(dataset_path):\n",
        "    class_names = set()\n",
        "    image_sizes = set()\n",
        "    extensions = set()\n",
        "\n",
        "    for class_folder in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_folder)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        class_names.add(class_folder)\n",
        "\n",
        "        for file in os.listdir(class_path):\n",
        "            if not file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue\n",
        "            extensions.add(os.path.splitext(file)[1].lower())\n",
        "\n",
        "            try:\n",
        "                with Image.open(os.path.join(class_path, file)) as img:\n",
        "                    image_sizes.add(img.size)\n",
        "            except:\n",
        "                pass  # skip unreadable files (already verified as clean earlier)\n",
        "\n",
        "    print(f\"\\n📂 Split: {dataset_path}\")\n",
        "    print(f\"Unique Class Names: {sorted(class_names)}\")\n",
        "    print(f\"Unique Image Sizes: {sorted(image_sizes)}\")\n",
        "    print(f\"Unique File Extensions: {sorted(extensions)}\")\n",
        "\n",
        "# Run for all splits\n",
        "check_unique_values(os.path.join(DATASET_DIR, 'train'))\n",
        "check_unique_values(os.path.join(DATASET_DIR, 'valid'))\n",
        "check_unique_values(os.path.join(DATASET_DIR, 'test'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Constants\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Training set with augmentation (next step)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Validation and Test sets — only rescaling\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=os.path.join(DATASET_DIR, 'train'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    directory=os.path.join(DATASET_DIR, 'valid'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    directory=os.path.join(DATASET_DIR, 'test'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPdeXF9aDg_Z"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Augmentation for training only\n",
        "train_datagen_aug = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation and test remain unchanged\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Augmented training generator\n",
        "train_generator = train_datagen_aug.flow_from_directory(\n",
        "    directory=os.path.join(DATASET_DIR, 'train'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Validation generator\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    directory=os.path.join(DATASET_DIR, 'valid'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Test generator\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    directory=os.path.join(DATASET_DIR, 'test'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "#Custom CNN Architecture (Simple and Effective)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Define the model\n",
        "custom_cnn = Sequential()\n",
        "\n",
        "# Convolution Block 1\n",
        "custom_cnn.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "custom_cnn.add(BatchNormalization())\n",
        "custom_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Convolution Block 2\n",
        "custom_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "custom_cnn.add(BatchNormalization())\n",
        "custom_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Convolution Block 3\n",
        "custom_cnn.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "custom_cnn.add(BatchNormalization())\n",
        "custom_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten & Fully Connected Layers\n",
        "custom_cnn.add(Flatten())\n",
        "custom_cnn.add(Dense(256, activation='relu'))\n",
        "custom_cnn.add(Dropout(0.5))\n",
        "custom_cnn.add(Dense(4, activation='softmax'))  # 4 classes\n",
        "\n",
        "# Compile the model\n",
        "custom_cnn.compile(optimizer='adam',\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "custom_cnn.summary()\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "# Model Training (Custom CNN)\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "checkpoint = ModelCheckpoint('best_custom_cnn.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# Training\n",
        "history = custom_cnn.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=len(val_generator),\n",
        "    epochs=30,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Visualize Training History\n",
        "# Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSXCqbocDjON"
      },
      "outputs": [],
      "source": [
        "# Evaluate on Test Set\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the best model\n",
        "model = load_model('best_custom_cnn.h5')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"\\n✅ Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Predict class probabilities\n",
        "y_pred_probs = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = test_generator.classes\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\n📊 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_labels, yticklabels=class_labels, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYWHY-LwWKdu"
      },
      "source": [
        "The project used a Custom Convolutional Neural Network (CNN) to classify brain MRI images into four categories: glioma, meningioma, pituitary, and no tumor. The model included convolutional and pooling layers for feature extraction, along with dropout and batch normalization to reduce overfitting.\n",
        "\n",
        "The model achieved a test accuracy of 69.11%, which shows it correctly classified about 7 out of 10 images. It performed very well on glioma and pituitary tumors, with high recall, meaning most cases were correctly identified. However, it struggled with meningioma, where recall was low, indicating frequent misclassification.\n",
        "\n",
        "The model's training history showed steady improvement, and evaluation metrics like precision, recall, and F1-score confirm that while the model is reliable for some tumor types, there's room for improvement—especially for underperforming classes.\n",
        "\n",
        "This custom CNN serves as a solid baseline, and future enhancement using transfer learning or data balancing techniques could lead to better accuracy and class-wise performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3U8v62pDfux"
      },
      "outputs": [],
      "source": [
        "# Transfer Learning – Build & Train Pretrained Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load base model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "base_model.trainable = False  # Freeze base\n",
        "\n",
        "# Add custom classification head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output = Dense(4, activation='softmax')(x)  # 4 classes\n",
        "\n",
        "# Build final model\n",
        "mobilenet_model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "mobilenet_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                        loss='categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "mobilenet_model.summary()\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_mobilenetv2.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Train model\n",
        "history_mobilenet = mobilenet_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwI73iWaRWWj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the saved best model\n",
        "mobilenet_model = load_model('best_mobilenetv2.h5')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = mobilenet_model.evaluate(test_generator)\n",
        "print(f\"\\n✅ Test Accuracy (MobileNetV2): {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred_probs = mobilenet_model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = test_generator.classes\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\n📊 Classification Report (MobileNetV2):\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - MobileNetV2\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.plot(history_mobilenet.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_mobilenet.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('MobileNetV2 Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Loss plot\n",
        "plt.plot(history_mobilenet.history['loss'], label='Train Loss')\n",
        "plt.plot(history_mobilenet.history['val_loss'], label='Validation Loss')\n",
        "plt.title('MobileNetV2 Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVs0gkWbSlB3"
      },
      "outputs": [],
      "source": [
        "# Model Comparison – Custom CNN vs MobileNetV2\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Training Accuracy\n",
        "plt.plot(history.history['val_accuracy'], label='Custom CNN', marker='o')\n",
        "plt.plot(history_mobilenet.history['val_accuracy'], label='MobileNetV2', marker='x')\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Validation Loss\n",
        "plt.plot(history.history['val_loss'], label='Custom CNN', marker='o')\n",
        "plt.plot(history_mobilenet.history['val_loss'], label='MobileNetV2', marker='x')\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "For this brain tumor classification project, I prioritized evaluation metrics that align with both clinical accuracy and positive business impact. While overall accuracy gives a general sense of the model’s performance, precision and recall were more critical in this healthcare context. Precision ensures that when the model predicts a tumor, it is likely correct—reducing unnecessary tests and anxiety. Recall (or sensitivity) is even more vital, as it reflects the model’s ability to correctly identify actual tumor cases—minimizing the risk of missing critical diagnoses. The F1-score was also considered, as it balances precision and recall, especially useful when class distributions are uneven. Together, these metrics support safe, cost-effective, and trustworthy deployment in medical environments, where both false positives and false negatives can have serious consequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "For the final prediction model, I selected MobileNetV2 (Transfer Learning) due to its superior performance compared to the custom CNN. MobileNetV2 achieved higher overall accuracy, precision, recall, and F1-scores, especially in detecting critical tumor types like glioma and pituitary tumors. Its lightweight architecture and pre-trained ImageNet weights allowed for faster convergence and better generalization on the medical dataset, even with limited training data. This makes it not only more reliable in terms of predictive power but also efficient for real-world deployment, particularly in resource-constrained healthcare settings such as mobile or embedded diagnostic tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "In this project, we implemented two models: a Custom Convolutional Neural Network (CNN) and a pretrained MobileNetV2 through Transfer Learning. These models were trained to classify brain MRI images into four categories: glioma, meningioma, no tumor, and pituitary tumor. Both models demonstrated progressively improving performance, with MobileNetV2 outperforming the custom CNN in terms of validation and test accuracy.\n",
        "\n",
        "However, we did not employ any feature importance or model explainability tools such as Grad-CAM, SHAP, or LIME in this project. The primary reason is that feature importance tools are typically more useful when manual interpretation of the input features (like pixels or regions in an image) is necessary for clinical or scientific insight. In our case, since the project objective focused on classification performance rather than region-wise interpretation of MRI scans, using model explainability tools was not essential.\n",
        "\n",
        "The models were treated as end-to-end learning systems where convolutional layers handled feature extraction automatically from raw pixel data. As our primary goal was accurate classification rather than explaining the internal decision-making logic of the model to a medical audience, feature importance was considered optional and thus was not applied in this work. Nonetheless, such tools can be integrated in the future if the model needs to be deployed in clinical settings where interpretability and transparency are critical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import tensorflow as tf\n",
        "\n",
        "# Wrap in custom class\n",
        "class BrainTumorClassifier:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, processed_image):  # processed_image = batch (e.g. shape (1, 224, 224, 3))\n",
        "        return self.model.predict(processed_image)\n",
        "\n",
        "# Instantiate wrapper\n",
        "classifier = BrainTumorClassifier(mobilenet_model)\n",
        "\n",
        "# Save with joblib\n",
        "joblib.dump(classifier, '/Users/rudraahuja/Desktop/mobilenet_classifier.pkl')\n",
        "print(\"✅ Model saved as mobilenet_classifier.pkl on Desktop\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9W7rnb1daSt"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('mobilenet_classifier.pkl')  # for .pkl\n",
        "files.download('best_mobilenetv2.h5')       # for .h5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('best_mobilenetv2.h5')\n",
        "\n",
        "# Image path (update if your directory is different)\n",
        "img_path = '/content/drive/MyDrive/Tumour/test/glioma/Tr-gl_0016_jpg.rf.99746694ea97fe0b73108832b462d48e.jpg'\n",
        "\n",
        "# Load image and preprocess\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = img_array / 255.0  # Normalize\n",
        "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "# Predict\n",
        "pred = model.predict(img_array)\n",
        "class_index = np.argmax(pred)\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "predicted_class = class_labels[class_index]\n",
        "\n",
        "print(f\"📷 Predicted Tumor Type: **{predicted_class}** with confidence: {np.max(pred)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        " Brain Tumor MRI Image Classification Project\n",
        "In this project, I successfully designed and implemented a deep learning pipeline to classify brain MRI images into four categories: glioma, meningioma, pituitary tumor, and no tumor. The entire process—from understanding the dataset to model evaluation and deployment—was executed with a clear objective of building a reliable and accurate medical imaging solution.\n",
        "\n",
        "I began by exploring and validating the dataset, ensuring proper structure, consistent resolution, and class distribution. With clean and organized image data, I applied preprocessing techniques such as normalization and resizing, followed by data augmentation strategies to increase the diversity and robustness of the training set.\n",
        "\n",
        "I developed two models:\n",
        "\n",
        "A Custom CNN, built from the ground up, which achieved a solid 69.1% accuracy on the test set.\n",
        "A Transfer Learning model using MobileNetV2, which performed significantly better, reaching a test accuracy of 79.3%. Leveraging pretrained ImageNet weights enhanced feature extraction and overall generalization.\n",
        "Using evaluation metrics like precision, recall, and F1-score, I assessed model performance in detail. The MobileNetV2 model demonstrated superior accuracy and reliability, especially in detecting glioma and no tumor cases, making it my final choice for deployment.\n",
        "\n",
        "While I did not use specific feature importance or explainability tools in this phase—since image classification models inherently learn spatial patterns—these can be integrated in future iterations using tools like Grad-CAM or LIME for interpretability.\n",
        "\n",
        "I saved the best-performing model and validated it on unseen images as a sanity check, confirming its readiness for real-world use. A user-friendly Streamlit application is being developed to allow MRI image upload and instant tumor prediction with confidence scores—bringing AI-driven diagnostic support closer to clinical use.\n",
        "\n",
        "Highlights:\n",
        "Developed and compared both custom and pretrained deep learning models.\n",
        "Achieved high classification accuracy with MobileNetV2.\n",
        "Ensured the system is scalable, modular, and ready for deployment.\n",
        "Demonstrated real-world applicability in medical imaging diagnostics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "H0kj-8xxnORC",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
